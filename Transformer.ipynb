{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141c0c9-261a-4252-a4e3-f1c828dc316f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Decoder import Decoder\n",
    "from EncoderTransformer import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfe4861-cfde-4ead-baa3-f7f8017b3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import cast,math,linalg,float32,ones,maximum,newaxis\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cfd376a-298d-4d18-9ce0-ccb057333503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(Model):\n",
    "    def __init__(self,enc_vocab_size,dec_vocab_size,enc_seq_len,dec_seq_len,h,d_k,d_v,d_model,d_ff_inner,n,rate,**kwargs):\n",
    "        super(TransformerModel,self).__init__(**kwargs)\n",
    "        \n",
    "        #setup encoder\n",
    "        self.encoder=Encoder(enc_vocab_size,enc_seq_len,h,d_k,d_v,d_model,d_ff_inner,n,rate)\n",
    "        \n",
    "        #setup decoder\n",
    "        self.decoder=Decoder(dec_vocab_size,dec_seq_len,h,d_k,d_v,d_model,d_ff_inner,n,rate)\n",
    "        \n",
    "        #final dense layer\n",
    "        self.model_last_layer=Dense(dec_vocab_size)\n",
    "        \n",
    "    def padding_mask(self,input):\n",
    "        #creating mask which marks zero padding values in the input by a 1.0\n",
    "        mask=math.equal(input,0)\n",
    "        mask=cast(mask,float32)\n",
    "        \n",
    "        #shape of the mask should be broadcastable to the shape of the attention weights that it will be masking later on\n",
    "        return mask[:,newaxis,newaxis,:]\n",
    "    \n",
    "    def lookahead_mask(self,shape):\n",
    "        #mask out future entries by marking them with a 1.0\n",
    "        mask=1-linalg.band_part(ones((shape,shape)),-1,0)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def call(self,encoder_input,decoder_input,training):\n",
    "        #creating padding mask to the mask encoder inputs and the encoder outputs in the decoder\n",
    "        enc_padding_mask=self.padding_mask(encoder_input)\n",
    "        \n",
    "        #create and combine padding and lookahead mask to be fed into the decoder\n",
    "        dec_in_padding_mask=self.padding_mask(decoder_input)\n",
    "        dec_in_lookahead_mask=self.lookahead_mask(decoder_input.shape[1])\n",
    "        dec_in_lookahead_mask=maximum(dec_in_padding_mask,dec_in_lookahead_mask)\n",
    "        \n",
    "        #feed the input into the enoder\n",
    "        encoder_output=self.encoder(input_seq=encoder_input,padding_mask=enc_padding_mask,training=training)\n",
    "        \n",
    "        #feed the encoder output into decoder\n",
    "        decoder_output=self.decoder(output_target=decoder_input,encoder_output=encoder_output,lookahead_mask=dec_in_lookahead_mask,padding_mask=enc_padding_mask,training=training)\n",
    "        \n",
    "        #pass the decoder output through a final dense layer\n",
    "        model_output=self.model_last_layer(decoder_output)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1dc8f3-277f-40d2-817f-164c6bdbf777",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "dec_vocab_size = 20 # Vocabulary size for the decoder\n",
    " \n",
    "enc_seq_length = 5  # Maximum length of the input sequence\n",
    "dec_seq_length = 5  # Maximum length of the target sequence\n",
    " \n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    " \n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883de7f-da72-4733-b84d-fa9eb7e7ece5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
