{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4f904ec7-cec3-4c91-a946-7b78005647f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import convert_to_tensor, string\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Layer,LayerNormalization,Dense,ReLU,Dropout\n",
    "import numpy as np\n",
    "from tensorflow import matmul,cast,float32,math,reshape,shape,transpose\n",
    "from tensorflow.keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "8e75f2cf-aeb0-4edf-8a71-b8ddad200efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        self.word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        self.position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            trainable=False,\n",
    "            name=\"word\",\n",
    "        )\n",
    "        ##fixed weights\n",
    "        self.word_embedding_layer.add_weight(shape=(vocab_size, output_dim),trainable=False)\n",
    "        self.word_embedding_layer.set_weights([self.word_embedding_matrix])\n",
    "        \n",
    "        \n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            trainable=False,\n",
    "            name=\"pos\",\n",
    "        )\n",
    "        ##fixed weights\n",
    "        self.position_embedding_layer.add_weight(shape=(sequence_length, output_dim),trainable=False)\n",
    "        self.position_embedding_layer.set_weights([self.position_embedding_matrix])\n",
    "    \n",
    "        \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "    \n",
    " \n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "636bfc4e-fe1b-467c-af0a-ca9a67e70672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNormalization(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(AddNormalization,self).__init__(**kwargs)\n",
    "        self.layer_norm=LayerNormalization()\n",
    "    \n",
    "    def call(self,x,sublayer_x):\n",
    "        #sublayer i/p and o/p need to be of the same shape to be summed\n",
    "        add=x+sublayer_x\n",
    "        \n",
    "        #apply layer normalization to the sum\n",
    "        return self.layer_norm(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "250f9041-79ca-4e06-983b-808260f8315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "    def __init__(self,d_ff,d_model,**kwargs):\n",
    "        super(FeedForward,self).__init__(**kwargs)\n",
    "        self.fully_connected1=Dense(d_ff) #first fully connected layer\n",
    "        self.fully_connected2=Dense(d_model) #second fully connected layer\n",
    "        self.activation=ReLU() #relu activation layer\n",
    "    \n",
    "    def call(self,x):\n",
    "        #the i/p passed into the 2 fully connected layers,with a relu in b/w\n",
    "        x_fc1=self.fully_connected1(x)\n",
    "        \n",
    "        return self.fully_connected2(self.activation(x_fc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "331d5d27-08e3-4fd0-a13e-90a43ce02557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(DotProductAttention,self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self,queries,keys,values,d_k,mask=None):\n",
    "        scores=matmul(queries,keys,transpose_b=True)/math.sqrt(cast(d_k,float32))\n",
    "    \n",
    "    #applying mask so as to not base the occurence of a word on the basis of the words ahead\n",
    "        if mask is not None:\n",
    "            scores+= -1e9*mask\n",
    "        weights=softmax(scores)\n",
    "        \n",
    "        return matmul(weights,values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "26bc134f-8678-49bd-9f9a-3b151de24c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self,h,d_k,d_v,d_model,**kwargs):\n",
    "        super(MultiHeadAttention,self).__init__(**kwargs)\n",
    "        self.attention=DotProductAttention() #scaled dot product attention\n",
    "        self.heads=h #number of attention heads to use\n",
    "        self.d_k=d_k #dimentionality of linearly projected queries and keys\n",
    "        self.d_v=d_v #dimantionality of linearly projected values\n",
    "        self.d_model=d_model #dimentionality of the model\n",
    "        self.W_q=Dense(d_k) #learned projection matrix for the queries\n",
    "        self.W_k=Dense(d_k) #learned projection matrix for the keys\n",
    "        self.W_v=Dense(d_v) #learned projection matrix for the values\n",
    "        self.W_o=Dense(d_model) #leanred projection matrix for the multi head o/p\n",
    "        \n",
    "    def reshape_tensor(self,x,heads,flag):\n",
    "        if flag:\n",
    "            #tensor shape after reshaping and transposing: (batch_size,heads,seq_length,-1)\n",
    "            x=reshape(x,shape=(shape(x)[0],shape(x)[1],heads,-1))\n",
    "            x=transpose(x,perm=(0,2,1,3))\n",
    "        else:\n",
    "            #reverting the reshaping and transposing opertaions:(batch_size,seq,length,d_k)\n",
    "            x=transpose(x,perm=(0,2,1,3))\n",
    "            x=reshape(x,shape=(shape(x)[0],shape(x)[1],self.d_k))\n",
    "        return x\n",
    "    \n",
    "    def call(self,queries,keys,values,mask=None):\n",
    "        #rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped=self.reshape_tensor(self.W_q(queries),self.heads,True)\n",
    "        #resulting tensor shape: (batch_size,heads,input_seq_len,-1)\n",
    "        \n",
    "        #rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped=self.reshape_tensor(self.W_k(keys),self.heads,True)\n",
    "        \n",
    "        #rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped=self.reshape_tensor(self.W_v(values),self.heads,True)\n",
    "        \n",
    "        #compute the multi head attention o/p using the reshaped q,k,v\n",
    "        o_reshaped=self.attention(queries=q_reshaped,keys=k_reshaped,values=v_reshaped,d_k=self.d_k,mask=mask)\n",
    "        #resulting tensor shape: (batch_size,input_seq_len,-1)\n",
    "        \n",
    "        #rearrange back the o/p into concatenated form\n",
    "        output=self.reshape_tensor(o_reshaped,self.heads,False)\n",
    "        #resulting tensor shape: (batch_size,heads,input_seq_len,d_k)\n",
    "        \n",
    "        #apply one final layer linear projection to the o/p to generate the mutlihead attention\n",
    "        #resulting tensor shaoe:(batch_size,input_seq_len,d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "04e74d2c-34bd-471b-8bb7-fc77b8141298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "    def __init__(self,h,d_k,d_v,d_model,d_ff,rate,**kwargs):\n",
    "        super(EncoderLayer,self).__init__(**kwargs)\n",
    "        self.multihead_attention=MultiHeadAttention(h,d_k,d_v,d_model)\n",
    "        self.dropout1=Dropout(rate)\n",
    "        self.add_norm1=AddNormalization()\n",
    "        self.feed_forward=FeedForward(d_ff,d_model)\n",
    "        self.dropout2=Dropout(rate)\n",
    "        self.add_norm2=AddNormalization()\n",
    "        \n",
    "    def call(self,x,padding_mask,training):\n",
    "        #multihead attention layer\n",
    "        multihead_output=self.multihead_attention(queries=x,keys=x,values=x,mask=padding_mask)\n",
    "        #expected o/p shape =(batch_size,seq_len,d_model)\n",
    "        \n",
    "        #dropout\n",
    "        multihead_output=self.dropout1(multihead_output,training=training)\n",
    "        \n",
    "        #Add and Norm Layer\n",
    "        addnorm_output=self.add_norm1(x,multihead_output)\n",
    "        #expected o/p shape=(batch_size,seq_len,d_model)\n",
    "        \n",
    "        #fully connected layer\n",
    "        feedforward_output=self.feed_forward(addnorm_output)\n",
    "        #expected shape=(batch_size,seq_len,d_model)\n",
    "        \n",
    "        #dropout layer\n",
    "        feedforward_output=self.dropout2(feedforward_output,training=training)\n",
    "        \n",
    "        #Add and Norm layer\n",
    "        return self.add_norm2(addnorm_output,feedforward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "bcfd71b2-1e56-42bd-954c-ab983a52c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self,vocab_size,seq_len,h,d_k,d_v,d_model,d_ff,n,rate,**kwargs):\n",
    "        super(Encoder,self).__init__(**kwargs)\n",
    "        self.pos_encoding=PositionEmbeddingFixedWeights(seq_len,vocab_size,d_model)\n",
    "        self.dropout=Dropout(rate)\n",
    "        self.encoder_layer=[EncoderLayer(h,d_k,d_v,d_model,d_ff,rate) for _ in range(n)]\n",
    "        \n",
    "    \n",
    "    def call(self,input_seq,padding_mask,training):\n",
    "        #generate positional encoding\n",
    "        pos_encoding_output=self.pos_encoding(input_seq)\n",
    "        \n",
    "        #expected output shape=(batch_size,seq_len,d_model)\n",
    "        \n",
    "        #droupout layer\n",
    "        x=self.dropout(pos_encoding_output,training=training)\n",
    "        \n",
    "        #passing positional encoded values to each encoder layer\n",
    "        for i ,layer in enumerate(self.encoder_layer):\n",
    "            x=layer(x,padding_mask=padding_mask,training=training)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdd596-2c16-4db4-a841-11ae606eb0d3",
   "metadata": {},
   "source": [
    "### testing using dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ec76bc70-3fa5-443e-97ef-ac2d62069178",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1.1147017   0.16441566  0.55740565 ... -0.37166512 -0.15431736\n",
      "   -1.8471124 ]\n",
      "  [ 0.7618469   0.8236067  -0.60659933 ... -1.2163182   0.3851881\n",
      "   -2.982336  ]\n",
      "  [ 1.3416651   0.04513863 -0.15659723 ... -0.5181147   0.32735035\n",
      "   -1.9324467 ]\n",
      "  [ 0.8564203   0.73370904 -0.33491606 ... -0.39158142  0.0924149\n",
      "   -1.8935617 ]\n",
      "  [ 0.80698574  0.12150733  0.06685011 ... -0.21014674 -0.41041982\n",
      "   -0.76314354]]\n",
      "\n",
      " [[ 0.32850823  0.3916108  -0.11228144 ...  0.24602424  0.05485585\n",
      "   -1.5224779 ]\n",
      "  [ 1.565122    0.06600136 -0.19128309 ... -0.41524026  0.95913863\n",
      "   -2.328098  ]\n",
      "  [ 0.12286402 -0.95155865 -0.8306134  ... -0.47583726 -0.47932354\n",
      "   -0.9740388 ]\n",
      "  [ 0.4104212   0.43118575 -0.3968854  ... -0.34344634 -0.2612015\n",
      "   -1.6174657 ]\n",
      "  [ 0.9022037   0.0748307  -0.4029294  ... -0.01195032 -0.3776412\n",
      "   -1.3287237 ]]\n",
      "\n",
      " [[ 1.195409    0.5425064   0.0361741  ... -0.6074726   0.91755617\n",
      "   -0.74038535]\n",
      "  [ 1.305232    0.8578337  -0.4308623  ... -0.66527903  0.90632313\n",
      "   -2.7432458 ]\n",
      "  [ 0.5188408  -0.35201353 -0.10730551 ... -0.60890955  0.4545176\n",
      "   -1.7724835 ]\n",
      "  [ 0.8446274   0.37425107  0.11495323 ...  0.02071382 -0.03070606\n",
      "   -1.9211828 ]\n",
      "  [ 1.3853967  -0.4573661  -1.119936   ... -0.24731018  0.37379026\n",
      "   -0.92011976]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.90723026  0.11345535  0.09352697 ... -0.1854772   0.17329298\n",
      "   -1.5984532 ]\n",
      "  [ 1.8203971   0.5222726  -0.5180275  ... -0.13540071  0.3542453\n",
      "   -1.7571009 ]\n",
      "  [ 1.1166546   0.48651847  0.12304006 ...  0.13098289  0.19504456\n",
      "   -0.35434985]\n",
      "  [ 0.6008037   0.5074839  -0.38371873 ... -0.08929159  0.01440891\n",
      "   -2.0488243 ]\n",
      "  [ 1.3359144   0.96185106 -0.5074751  ... -0.37398303 -0.0711356\n",
      "   -1.4564213 ]]\n",
      "\n",
      " [[ 1.2125937   0.7218313   0.9359556  ...  0.26292127 -0.5095297\n",
      "   -1.6713871 ]\n",
      "  [ 0.84019625  0.5691589   0.01826695 ... -0.29815862 -0.450699\n",
      "   -2.0281713 ]\n",
      "  [ 1.4888309  -0.46368256 -0.1478863  ...  0.04835742 -0.21892726\n",
      "   -1.6287513 ]\n",
      "  [ 0.6230562   0.7329346  -0.50934476 ... -0.07926901 -0.76732904\n",
      "   -0.54616576]\n",
      "  [ 1.2070903  -0.04622645 -1.0550911  ... -0.3141328  -0.66329896\n",
      "   -1.9596435 ]]\n",
      "\n",
      " [[ 1.1951909   0.39835557  0.51235735 ... -0.07208423 -0.18288901\n",
      "   -1.8892244 ]\n",
      "  [ 0.7686494   0.77072793  0.19708908 ... -0.5730996   1.0036395\n",
      "   -2.5485983 ]\n",
      "  [ 0.83462316  0.42472363 -0.03705603 ... -0.08515334 -0.45589995\n",
      "   -2.558007  ]\n",
      "  [ 1.0202156   0.3986013   0.4132931  ... -0.3227581   0.15898603\n",
      "   -2.4324563 ]\n",
      "  [ 0.8810968   0.7171309  -0.35262644 ... -0.59774584 -0.17399418\n",
      "   -1.9052403 ]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "enc_vocab_size=20 #vocabulary size for the encoder\n",
    "input_seq_len=5 #max size of the i/p seq\n",
    "h=8 #number of self-attention heads\n",
    "d_k=64 #dimentionality of the linearly projected queris and keys\n",
    "d_v=64 #dimentionality of the linearly projected values\n",
    "d_ff=2048 #dimentionality of the inner fully connected layer\n",
    "d_model=512 #dimentionality of the model sub-layers' o/p\n",
    "n=6 #numer of layers in the encoder stack\n",
    "\n",
    "batch_size=64 #batch size form the the training process\n",
    "dropout_rate=0.1 #frequency of dropping the i/p units in the dropout layer\n",
    "input_seq=random.random((batch_size,input_seq_len))\n",
    "\n",
    "encoder=Encoder(enc_vocab_size,input_seq_len,h,d_k,d_v,d_model,d_ff,n,dropout_rate)\n",
    "print(encoder(input_seq,padding_mask=None,training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de9173-6191-4896-bd24-0314f33a3d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
