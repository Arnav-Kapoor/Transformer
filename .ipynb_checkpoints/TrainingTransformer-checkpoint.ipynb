{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1457a7f-c324-462f-ab03-3c6fe0ad694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data used is already cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4344bf0f-2c94-45a7-9a0f-40efb081c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701129e8-4310-4c20-afb8-fd94392c1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self,**kwargs):\n",
    "        super(PrepareDataset,self).__init__(**kwargs)\n",
    "        self.n_sentences=10000 #number of sentences to include in the dataset\n",
    "        self.train_split=0.9 #ratio of training data split\n",
    "    \n",
    "    #fit tokenizer\n",
    "    def create_tokenizer(self,dataset):\n",
    "        tokenizer=Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return tokenizer\n",
    "    \n",
    "    def find_seq_length(self,dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "    \n",
    "    def find_vocab_size(self,tokenizer,dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        \n",
    "        return len(tokenizer.word_index)+1\n",
    "    \n",
    "    def __call__(self,filename,**kwargs):\n",
    "        #load clean dataset\n",
    "        clean_dataset=load(open(filename,'rb'))\n",
    "        \n",
    "        #reduce dataset size\n",
    "        dataset=clean_dataset[:self.n_sentences,:]\n",
    "        \n",
    "        #include start and end of string tokens\n",
    "        for i in range(dataset[:,0].size):\n",
    "            dataset[i,0]=\"<START> \"+dataset[i,0]+\" <EOS>\"\n",
    "            dataset[i,1]=\"<START> \"+dataset[i,0]+\" <EOS>\"\n",
    "        \n",
    "        #random shuffle the dataset\n",
    "        shuffle(dataset)\n",
    "        \n",
    "        #split the dataset\n",
    "        train=dataset[:int(self.n_sentences*self.train_split)]\n",
    "        \n",
    "        #prepare tokenizer for the encoder input\n",
    "        enc_tokenizer=self.create_tokenizer(train[:,0])\n",
    "        enc_seq_length=self.find_seq_length(train[:,0])\n",
    "        enc_vocab_size=self.find_vocab_size(enc_tokenizer,train[:,0])\n",
    "        \n",
    "        #encode and pad the input sequences\n",
    "        trainX=enc_tokenizer.texts_to_sequences(train[:,0])\n",
    "        trainX=pad_sequences(trainX,maxlen=enc_seq_length,padding='post')\n",
    "        trainX=convert_to_tensor(trainX,dtype=int64)\n",
    "        \n",
    "        #prepare dataset for decoder input\n",
    "        dec_tokenizer=self.create_tokenizer(train[:,1])\n",
    "        dec_seq_length=self.find_seq_length(train[:,1])\n",
    "        dec_vocab_size=self.find_vocab_size(dec_tokenizer,train[:,1])\n",
    "        \n",
    "        #encode and pad the input sequences\n",
    "        trainY=dec_tokenizer.texts_to_sequences(train[:,1])\n",
    "        trainY=pad_sequences(trainY,maxlen=dec_seq_length,padding='post')\n",
    "        trainY=convert_to_tensor(trainY,dtype=int64)\n",
    "        \n",
    "        return trainX,trainY,train,enc_seq_length,dec_seq_length,enc_vocab_size,dec_vocab_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad6c069-6e19-4101-a68d-57730010a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow import data,train,math,reduce_sum,cast,equal,argmax,float32,GradientTape,TensorSpec,function,int64\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690970c-1b57-4101-bdc9-441a4c87a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1364c0e-9888-475d-b6d5-8c13b972a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    " \n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4088b144-93bd-4a3d-bcf4-39c467c3cbb3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Epoch 1 step 0 loss 11.1791 Accuracy 0.0000\n",
      "Epoch 1: Training Loss 11.1791, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.9301, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.8695, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.8142, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.7545, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.6756, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.6545, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.6762, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.6628, Training Accuracy 0.0000\n",
      "Epoch 1: Training Loss 10.5994, Training Accuracy 0.0002\n",
      "Epoch 1: Training Loss 10.5865, Training Accuracy 0.0002\n",
      "Epoch 1: Training Loss 10.5557, Training Accuracy 0.0002\n",
      "Epoch 1: Training Loss 10.5271, Training Accuracy 0.0002\n",
      "Epoch 1: Training Loss 10.5028, Training Accuracy 0.0002\n",
      "Epoch 1: Training Loss 10.4607, Training Accuracy 0.0002\n",
      "Epoch 1: Training Loss 10.4220, Training Accuracy 0.0003\n",
      "Epoch 1: Training Loss 10.3780, Training Accuracy 0.0003\n",
      "Epoch 1: Training Loss 10.3404, Training Accuracy 0.0003\n",
      "Epoch 1: Training Loss 10.3019, Training Accuracy 0.0003\n",
      "Epoch 1: Training Loss 10.2589, Training Accuracy 0.0003\n",
      "Epoch 1: Training Loss 10.2153, Training Accuracy 0.0006\n",
      "Epoch 1: Training Loss 10.1653, Training Accuracy 0.0009\n",
      "Epoch 1: Training Loss 10.1215, Training Accuracy 0.0027\n",
      "Epoch 1: Training Loss 10.0733, Training Accuracy 0.0060\n",
      "Epoch 1: Training Loss 10.0269, Training Accuracy 0.0119\n",
      "Epoch 1: Training Loss 9.9802, Training Accuracy 0.0176\n",
      "Epoch 1: Training Loss 9.9433, Training Accuracy 0.0259\n",
      "Epoch 1: Training Loss 9.8927, Training Accuracy 0.0334\n",
      "Epoch 1: Training Loss 9.8442, Training Accuracy 0.0406\n",
      "Epoch 1: Training Loss 9.7926, Training Accuracy 0.0470\n",
      "Epoch 1: Training Loss 9.7431, Training Accuracy 0.0533\n",
      "Epoch 1: Training Loss 9.6945, Training Accuracy 0.0590\n",
      "Epoch 1: Training Loss 9.6406, Training Accuracy 0.0638\n",
      "Epoch 1: Training Loss 9.5830, Training Accuracy 0.0689\n",
      "Epoch 1: Training Loss 9.5277, Training Accuracy 0.0739\n",
      "Epoch 1: Training Loss 9.4734, Training Accuracy 0.0782\n",
      "Epoch 1: Training Loss 9.4178, Training Accuracy 0.0821\n",
      "Epoch 1: Training Loss 9.3632, Training Accuracy 0.0866\n",
      "Epoch 1: Training Loss 9.3077, Training Accuracy 0.0903\n",
      "Epoch 1: Training Loss 9.2534, Training Accuracy 0.0940\n",
      "Epoch 1: Training Loss 9.1994, Training Accuracy 0.0976\n",
      "Epoch 1: Training Loss 9.1444, Training Accuracy 0.1021\n",
      "Epoch 1: Training Loss 9.0906, Training Accuracy 0.1065\n",
      "Epoch 1: Training Loss 9.0379, Training Accuracy 0.1103\n",
      "Epoch 1: Training Loss 8.9854, Training Accuracy 0.1147\n",
      "Epoch 1: Training Loss 8.9329, Training Accuracy 0.1183\n",
      "Epoch 1: Training Loss 8.8808, Training Accuracy 0.1218\n",
      "Epoch 1: Training Loss 8.8292, Training Accuracy 0.1251\n",
      "Epoch 1: Training Loss 8.7778, Training Accuracy 0.1286\n",
      "Epoch 1: Training Loss 8.7264, Training Accuracy 0.1315\n",
      "Epoch 1 step 50 loss 8.6763 Accuracy 0.1339\n",
      "Epoch 1: Training Loss 8.6763, Training Accuracy 0.1339\n",
      "Epoch 1: Training Loss 8.6266, Training Accuracy 0.1364\n",
      "Epoch 1: Training Loss 8.5784, Training Accuracy 0.1388\n",
      "Epoch 1: Training Loss 8.5309, Training Accuracy 0.1413\n",
      "Epoch 1: Training Loss 8.4832, Training Accuracy 0.1434\n",
      "Epoch 1: Training Loss 8.4363, Training Accuracy 0.1459\n",
      "Epoch 1: Training Loss 8.3903, Training Accuracy 0.1474\n",
      "Epoch 1: Training Loss 8.3449, Training Accuracy 0.1492\n",
      "Epoch 1: Training Loss 8.3003, Training Accuracy 0.1511\n",
      "Epoch 1: Training Loss 8.2561, Training Accuracy 0.1527\n",
      "Epoch 1: Training Loss 8.2118, Training Accuracy 0.1547\n",
      "Epoch 1: Training Loss 8.1687, Training Accuracy 0.1569\n",
      "Epoch 1: Training Loss 8.1261, Training Accuracy 0.1598\n",
      "Epoch 1: Training Loss 8.0837, Training Accuracy 0.1628\n",
      "Epoch 1: Training Loss 8.0421, Training Accuracy 0.1666\n",
      "Epoch 1: Training Loss 8.0014, Training Accuracy 0.1706\n",
      "Epoch 1: Training Loss 7.9608, Training Accuracy 0.1746\n",
      "Epoch 1: Training Loss 7.9207, Training Accuracy 0.1786\n",
      "Epoch 1: Training Loss 7.8812, Training Accuracy 0.1822\n",
      "Epoch 1: Training Loss 7.8422, Training Accuracy 0.1857\n",
      "Epoch 1: Training Loss 7.8025, Training Accuracy 0.1893\n",
      "Epoch 1: Training Loss 7.7647, Training Accuracy 0.1928\n",
      "Epoch 1: Training Loss 7.7266, Training Accuracy 0.1966\n",
      "Epoch 1: Training Loss 7.6893, Training Accuracy 0.2001\n",
      "Epoch 1: Training Loss 7.6514, Training Accuracy 0.2037\n",
      "Epoch 1: Training Loss 7.6144, Training Accuracy 0.2071\n",
      "Epoch 1: Training Loss 7.5779, Training Accuracy 0.2107\n",
      "Epoch 1: Training Loss 7.5416, Training Accuracy 0.2140\n",
      "Epoch 1: Training Loss 7.5051, Training Accuracy 0.2174\n",
      "Epoch 1: Training Loss 7.4692, Training Accuracy 0.2204\n",
      "Epoch 1: Training Loss 7.4344, Training Accuracy 0.2232\n",
      "Epoch 1: Training Loss 7.3998, Training Accuracy 0.2260\n",
      "Epoch 1: Training Loss 7.3650, Training Accuracy 0.2289\n",
      "Epoch 1: Training Loss 7.3306, Training Accuracy 0.2316\n",
      "Epoch 1: Training Loss 7.2964, Training Accuracy 0.2345\n",
      "Epoch 1: Training Loss 7.2628, Training Accuracy 0.2372\n",
      "Epoch 1: Training Loss 7.2295, Training Accuracy 0.2399\n",
      "Epoch 1: Training Loss 7.1958, Training Accuracy 0.2429\n",
      "Epoch 1: Training Loss 7.1628, Training Accuracy 0.2454\n",
      "Epoch 1: Training Loss 7.1303, Training Accuracy 0.2481\n",
      "Epoch 1: Training Loss 7.0992, Training Accuracy 0.2502\n",
      "Epoch 1: Training Loss 7.0690, Training Accuracy 0.2522\n",
      "Epoch 1: Training Loss 7.0375, Training Accuracy 0.2546\n",
      "Epoch 1: Training Loss 7.0061, Training Accuracy 0.2570\n",
      "Epoch 1: Training Loss 6.9758, Training Accuracy 0.2593\n",
      "Epoch 1: Training Loss 6.9456, Training Accuracy 0.2616\n",
      "Epoch 1: Training Loss 6.9157, Training Accuracy 0.2637\n",
      "Epoch 1: Training Loss 6.8850, Training Accuracy 0.2662\n",
      "Epoch 1: Training Loss 6.8569, Training Accuracy 0.2682\n",
      "Epoch 1: Training Loss 6.8278, Training Accuracy 0.2703\n",
      "Epoch 1 step 100 loss 6.7995 Accuracy 0.2725\n",
      "Epoch 1: Training Loss 6.7995, Training Accuracy 0.2725\n",
      "Epoch 1: Training Loss 6.7723, Training Accuracy 0.2745\n",
      "Epoch 1: Training Loss 6.7441, Training Accuracy 0.2767\n",
      "Epoch 1: Training Loss 6.7171, Training Accuracy 0.2787\n",
      "Epoch 1: Training Loss 6.6910, Training Accuracy 0.2807\n",
      "Epoch 1: Training Loss 6.6647, Training Accuracy 0.2828\n",
      "Epoch 1: Training Loss 6.6393, Training Accuracy 0.2850\n",
      "Epoch 1: Training Loss 6.6140, Training Accuracy 0.2870\n",
      "Epoch 1: Training Loss 6.5880, Training Accuracy 0.2892\n",
      "Epoch 1: Training Loss 6.5636, Training Accuracy 0.2911\n",
      "Epoch 1: Training Loss 6.5390, Training Accuracy 0.2930\n",
      "Epoch 1: Training Loss 6.5140, Training Accuracy 0.2950\n",
      "Epoch 1: Training Loss 6.4893, Training Accuracy 0.2971\n",
      "Epoch 1: Training Loss 6.4650, Training Accuracy 0.2992\n",
      "Epoch 1: Training Loss 6.4421, Training Accuracy 0.3010\n",
      "Epoch 1: Training Loss 6.4180, Training Accuracy 0.3030\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m decoder_input\u001b[38;5;241m=\u001b[39mtrain_batchY[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    105\u001b[0m decoder_output\u001b[38;5;241m=\u001b[39mtrain_batchY[:,\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 107\u001b[0m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#implementing a learning rate scheduler\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self,d_model,warmup_steps=4000,**kwargs):\n",
    "        super(LRScheduler,self).__init__(*kwargs)\n",
    "        \n",
    "        self.d_model=cast(d_model,float32)\n",
    "        self.warmup_steps=warmup_steps\n",
    "    \n",
    "    def __call__(self,step_num):\n",
    "        #linearly increasing the learning rate for the first warmup_steps and decreasing it thereafter\n",
    "        # print(type(step_num))\n",
    "        arg1=cast(step_num,float32)**-0.5\n",
    "        # arg1=1.0/(step_num** 0.5)\n",
    "        arg2=cast(step_num,float32)*(1.0/(self.warmup_steps**1.5))\n",
    "        \n",
    "        return (1.0/(self.d_model**0.5))*math.minimum(arg1,arg2)\n",
    "    \n",
    "#instantiate an adam optimizer\n",
    "optimizer=Adam(LRScheduler(d_model),beta_1,beta_2,epsilon)\n",
    "\n",
    "#prepare the training and test splits of the dataset\n",
    "dataset=PrepareDataset()\n",
    "trainX,trainY,train_orig,enc_seq_length,dec_seq_length,enc_vocab_size,dec_vocab_size=dataset('english-german-both.pkl')\n",
    "\n",
    "#prepare dataset batches\n",
    "train_dataset=data.Dataset.from_tensor_slices((trainX,trainY))\n",
    "train_dataset=train_dataset.batch(batch_size)\n",
    "\n",
    "#create model\n",
    "training_model=TransformerModel(enc_vocab_size,dec_vocab_size,enc_seq_length,dec_seq_length,h,d_k,d_v,d_model,d_ff,n,dropout_rate)\n",
    "\n",
    "#defining the loss function\n",
    "\n",
    "def loss_fcn(target,prediction):\n",
    "    #create mask so that the zero padding values are not included in the computation of loss\n",
    "    padding_mask=math.logical_not(equal(target,0))\n",
    "    padding_mask=cast(padding_mask,float32)\n",
    "    \n",
    "    #compute a sparse categorical cross_entropy loss on the unmasked values\n",
    "    loss=sparse_categorical_crossentropy(target,prediction,from_logits=True)\n",
    "    \n",
    "    #compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss)/reduce_sum(padding_mask)\n",
    "\n",
    "#defining the accuracy function\n",
    "def accuracy_fcn(target,prediction):\n",
    "    #create mask so that the zero padding values are not included in the computaion of accuracy\n",
    "    padding_mask=math.logical_not(equal(target,0))\n",
    "    \n",
    "    #find equal prediction and target values, and apply the padding mask\n",
    "    accuracy=equal(target,argmax(prediction,axis=2))\n",
    "    accuracy=math.logical_and(padding_mask,accuracy)\n",
    "    \n",
    "    #cast true/false values to 32-bit precision floating-point numbers\n",
    "    padding_mask=cast(padding_mask,float32)\n",
    "    accuracy=cast(accuracy,float32)\n",
    "    \n",
    "    #compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy)/reduce_sum(padding_mask)\n",
    "\n",
    "#include metrics monitoring\n",
    "train_loss=Mean(name='train_loss')\n",
    "train_accuracy=Mean(name='train_accuracy')\n",
    "\n",
    "#create a checkpoint object and manager to manage multiple checkpoint\n",
    "ckpt=train.Checkpoint(model=training_model,optimizer=optimizer)\n",
    "ckpt_manager=train.CheckpointManager(ckpt,\"checkpoints\",max_to_keep=3)\n",
    "\n",
    "#speeding up the training process\n",
    "\n",
    "@function\n",
    "def train_step(encoder_input,decoder_input,decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        #run the forward pass of the model to generate prediciton\n",
    "        prediction=training_model(encoder_input=encoder_input,decoder_input=decoder_input,training=True)\n",
    "        \n",
    "        #compute the training loss\n",
    "        loss=loss_fcn(decoder_output,prediction)\n",
    "        \n",
    "        #compute the training accuracy\n",
    "        accuracy=accuracy_fcn(decoder_output,prediction)\n",
    "    \n",
    "    #retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients=tape.gradient(loss,training_model.trainable_variables)\n",
    "    \n",
    "    #update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients,training_model.trainable_weights))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "    \n",
    "    print(\"\\nStart of epoch %d\"%(epoch+1))\n",
    "    start_time=time()\n",
    "    \n",
    "    for step, (train_batchX,train_batchY) in enumerate(train_dataset):\n",
    "        \n",
    "        #define the encoder and decoder inputs and the decoder output\n",
    "        encoder_input=train_batchX[:,1:]\n",
    "        decoder_input=train_batchY[:,:-1]\n",
    "        decoder_output=train_batchY[:,1:]\n",
    "        \n",
    "        train_step(encoder_input=encoder_input,decoder_input=decoder_input,decoder_output=decoder_output)\n",
    "        \n",
    "        if step%50==0:\n",
    "            print(f'Epoch {epoch+1} step {step} loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "        \n",
    "        #print epoch number and loss value at end of every epoch\n",
    "        print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n",
    "        \n",
    "        #save a checkpoint after every five epochs\n",
    "        if(epoch+1)%5==0:\n",
    "            save_path=ckpt_manager.save()\n",
    "            print(\"saved checkpoint at epoch %d\"%(epoch+1))\n",
    "\n",
    "\n",
    "print(\"total time taken: %.2fs\" %(time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ab0d6-767e-44e9-b866-95b4dc4a09f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
