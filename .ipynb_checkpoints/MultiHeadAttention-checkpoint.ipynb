{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de851659-b4c2-4d2a-9a86-c2757017e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from tensorflow.keras.backend import softmax\n",
    "from tensorflow import matmul,cast,float32,math,reshape,shape,transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9ee942-6245-4898-a545-8191a3f8083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(DotProductAttention,self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self,queries,keys,values,d_k,mask=None):\n",
    "        scores=matmul(queries,keys,transpose_b=True)/math.sqrt(cast(d_k,float32))\n",
    "    \n",
    "    #applying mask so as to not base the occurence of a word on the basis of the words ahead\n",
    "        if mask is not None:\n",
    "            scores+= -1e9*mask\n",
    "        weights=softmax(scores)\n",
    "        \n",
    "        return matmul(weights,values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c3ee5f-2834-4ec4-90d9-fc9c7d9eb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self,h,d_k,d_v,d_model,**kwargs):\n",
    "        super(MultiHeadAttention,self).__init__(**kwargs)\n",
    "        self.attention=DotProductAttention() #scaled dot product attention\n",
    "        self.heads=h #number of attention heads to use\n",
    "        self.d_k=d_k #dimentionality of linearly projected queries and keys\n",
    "        self.d_v=d_v #dimantionality of linearly projected values\n",
    "        self.d_model=d_model #dimentionality of the model\n",
    "        self.W_q=Dense(d_k) #learned projection matrix for the queries\n",
    "        self.W_k=Dense(d_k) #learned projection matrix for the keys\n",
    "        self.W_v=Dense(d_v) #learned projection matrix for the values\n",
    "        self.W_o=Dense(d_model) #leanred projection matrix for the multi head o/p\n",
    "        \n",
    "    def reshape_tensor(self,x,heads,flag):\n",
    "        if flag:\n",
    "            #tensor shape after reshaping and transposing: (batch_size,heads,seq_length,-1)\n",
    "            x=reshape(x,shape=(shape(x)[0],shape(x)[1],heads,-1))\n",
    "            x=transpose(x,perm=(0,2,1,3))\n",
    "        else:\n",
    "            #reverting the reshaping and transposing opertaions:(batch_size,seq,length,d_k)\n",
    "            x=transpose(x,perm=(0,2,1,3))\n",
    "            x=reshape(x,shape=(shape(x)[0],shape(x)[1],self.d_k))\n",
    "        return x\n",
    "    \n",
    "    def call(self,queries,keys,values,mask=None):\n",
    "        #rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped=self.reshape_tensor(self.W_q(queries),self.heads,True)\n",
    "        #resulting tensor shape: (batch_size,heads,input_seq_len,-1)\n",
    "        \n",
    "        #rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped=self.reshape_tensor(self.W_k(keys),self.heads,True)\n",
    "        \n",
    "        #rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped=self.reshape_tensor(self.W_v(values),self.heads,True)\n",
    "        \n",
    "        #compute the multi head attention o/p using the reshaped q,k,v\n",
    "        o_reshaped=self.attention(q_reshaped,k_reshaped,v_reshaped,self.d_k,mask)\n",
    "        #resulting tensor shape: (batch_size,input_seq_len,-1)\n",
    "        \n",
    "        #rearrange back the o/p into concatenated form\n",
    "        output=self.reshape_tensor(o_reshaped,self.heads,False)\n",
    "        #resulting tensor shape: (batch_size,heads,input_seq_len,d_k)\n",
    "        \n",
    "        #apply one final layer linear projection to the o/p to generate the mutlihead attention\n",
    "        #resulting tensor shaoe:(batch_size,input_seq_len,d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ac324-79be-452f-840b-d7881854c2b6",
   "metadata": {},
   "source": [
    "### testing using dummy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e093db48-e56d-4d5e-9ea5-7b3ea0f91e37",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.0434286   0.02936906  0.05576456 ... -0.56693375 -0.19152977\n",
      "   -0.21011189]\n",
      "  [-0.04031079  0.03237865  0.05654873 ... -0.5679413  -0.18873364\n",
      "   -0.20874096]\n",
      "  [-0.03539615  0.02926255  0.05997868 ... -0.56486785 -0.1920926\n",
      "   -0.20644829]\n",
      "  [-0.04033906  0.03066664  0.0610002  ... -0.5660929  -0.19030315\n",
      "   -0.21040261]\n",
      "  [-0.03978018  0.02913226  0.05819973 ... -0.56733584 -0.19054492\n",
      "   -0.2096827 ]]\n",
      "\n",
      " [[-0.0372023  -0.058926    0.12408636 ... -0.51994145 -0.11749138\n",
      "   -0.14175162]\n",
      "  [-0.0357124  -0.06040482  0.12341858 ... -0.5191991  -0.11573078\n",
      "   -0.1413696 ]\n",
      "  [-0.03682811 -0.05871838  0.12145889 ... -0.5193449  -0.11685009\n",
      "   -0.14421883]\n",
      "  [-0.03612378 -0.05992437  0.12749702 ... -0.51628196 -0.11853358\n",
      "   -0.14406869]\n",
      "  [-0.03476172 -0.05852611  0.12721835 ... -0.5173383  -0.1182031\n",
      "   -0.14478958]]\n",
      "\n",
      " [[ 0.09031312  0.01680743  0.1951084  ... -0.5136962  -0.18819906\n",
      "   -0.13954961]\n",
      "  [ 0.09156879  0.01353643  0.19411293 ... -0.5133157  -0.18917224\n",
      "   -0.14098775]\n",
      "  [ 0.09059671  0.01679023  0.192057   ... -0.5149099  -0.18726669\n",
      "   -0.13830572]\n",
      "  [ 0.09366629  0.01617883  0.19017442 ... -0.51234    -0.19072731\n",
      "   -0.13959081]\n",
      "  [ 0.08927032  0.01604287  0.19007145 ... -0.51388294 -0.1874889\n",
      "   -0.13976279]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.07595703  0.02391968  0.10703777 ... -0.6095809  -0.19014902\n",
      "   -0.1414299 ]\n",
      "  [ 0.07905677  0.02669007  0.10491031 ... -0.6085849  -0.19087099\n",
      "   -0.14595692]\n",
      "  [ 0.07447588  0.02742667  0.10648084 ... -0.6067968  -0.1896162\n",
      "   -0.14247465]\n",
      "  [ 0.07570746  0.02575633  0.10695846 ... -0.60610706 -0.19188792\n",
      "   -0.14072888]\n",
      "  [ 0.07510994  0.02962273  0.10836073 ... -0.60716057 -0.18962196\n",
      "   -0.14194398]]\n",
      "\n",
      " [[-0.0014032  -0.03223916  0.03733523 ... -0.49976593 -0.1561984\n",
      "   -0.06805982]\n",
      "  [-0.00242919 -0.02865759  0.03769322 ... -0.5001036  -0.15464543\n",
      "   -0.06808764]\n",
      "  [-0.0018878  -0.03141327  0.03862305 ... -0.5011825  -0.15351863\n",
      "   -0.06890441]\n",
      "  [-0.00372344 -0.03112011  0.0392606  ... -0.50050294 -0.15509923\n",
      "   -0.06532554]\n",
      "  [-0.0022153  -0.02728571  0.0375643  ... -0.5031847  -0.15574802\n",
      "   -0.06429785]]\n",
      "\n",
      " [[ 0.10908279  0.04395893  0.05400935 ... -0.5861939  -0.27978083\n",
      "   -0.17343415]\n",
      "  [ 0.10686557  0.04495141  0.05423665 ... -0.5838766  -0.2793622\n",
      "   -0.17247932]\n",
      "  [ 0.10722906  0.04555165  0.05412748 ... -0.5849554  -0.27961317\n",
      "   -0.17204213]\n",
      "  [ 0.10987076  0.04384227  0.05346875 ... -0.588237   -0.28005514\n",
      "   -0.1748234 ]\n",
      "  [ 0.10853119  0.0458743   0.05423592 ... -0.5864996  -0.27976984\n",
      "   -0.17330396]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "input_seq_len=5 #max len of the i/p sequence\n",
    "h=8 #number of attention heads\n",
    "d_k=64 #dimentionility of the linearly projected queris and keys\n",
    "d_v=64 #dinetionality of the linearly projected values\n",
    "d_model=512 #dimentionality of the model sub-layers' o/p\n",
    "batch_size=64 #batch size from the training process\n",
    "\n",
    "queries=random.random((batch_size,input_seq_len,d_k))\n",
    "keys=random.random((batch_size,input_seq_len,d_k))\n",
    "values=random.random((batch_size,input_seq_len,d_v))\n",
    "\n",
    "multihead_attention=MultiHeadAttention(h,d_k,d_v,d_model)\n",
    "print(multihead_attention(queries,keys,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce0109-df1c-45ec-89ba-08c722596918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
